---
title: '6372: Unit 9 Homework'
author: "Matt Farrow"
date: ''
output:
 html_document:
 df_print: paged
 word_document: default
 pdf_document: default
fig_caption: yes
editor_options:
 chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## PCA Conceptual questions

1. TRUE/FALSE Principle component analysis is a predictive modeling technique such as linear regression and LDA. 
 
 $\qquad$ <span style="color: blue;">FALSE</span>
 
2. TRUE or FALSE? Technically speaking, PCA should not be applied to categorical variables. 
 
 $\qquad$ <span style="color: blue;">TRUE</span>
 
3. An analyst conducts a PCA on continuous variables 1 through 20 and settled on reducing the variables down to 4. The analyst then proceeds to conduct a linear regression using the 4 PC components as predictors and the response is variable 1. Why is this a horrible idea? 

$\qquad$ <span style="color: blue;">Because variable 1 is also included in the PC components, it cannot then be used as a response variable to the linear regression.</span>
 
4. Why is it important to conduct PCA on standardized variables (aka using the correlation matrix)?

$\qquad$ <span style="color: blue;">Standardizing variables makes the variables comparable and is especially important when the variables are in different scales.</span>

## Exercise #1 PCA Basics

The example conducted in class did not do a very good job of illustrating the interpretation goals of PCA. For this reason, we will switch to a baseball data set to go over the basics and play around with interpretation. The baseball data set is located in the Lahman package. The data set is quite comprehensive having baseball player statistics dating back to 1871. We are going to examine the earliest year, 2016, by itself. Lets take a quick summary to see what variables we have. 

```{r}
library(Lahman)

# Get data from Batting data set for 2016
bat_16 <- tibble(Batting %>% 
  filter(yearID == 2016))

# Look at the summary statistics
summary(bat_16)
```

For those of you who do not know too much about baseball, the first 5 variables are just general information, G is the number of games played while the rest are information for players batting ability. G is games, AB is number of batting attempts, R-Runs, H-hits, X2B and X3B are doubles and triples, HR-home runs, RBI-Runs Batted In. These are all general stats on how well the batters can hit the ball. SB (stolen bases) and CS (caught stealing) are statistics about a players ability to run the bases. BB and IBB are when the batter gets a walk. The other stats are HBP (hit by a pitch), SH (sacrifice hit) and SF (sacrifice fly), and GIDP (grounded into double play).

Sports data sets lend themselves well to PCA. We will use this example to go through similar concepts discussed in class. For starters let's start off with just a few variables in the set to verify PCA is doing what we expect it to. Here is a quick scatter plot matrix. The variables here are highly correlated with each other.

```{r}
# Create a reduced data set that contains only columns 6:10
reduced <- bat_16 %>% 
  select(6:10)

# Create a pairs plot
library(GGally)
reduced %>% 
  ggpairs() +
  theme_minimal()
```

Let's take a quick look at the summary statistics and in particular let's calculate the variance of each variable and add them up to obtain the total variance.

```{r}
# Summary statistics by variable
apply(reduced, 2, summary)

# Total variance of each variable
apply(reduced, 2, var)

# Total variance
sum(apply(reduced, 2, var))
```

We have been talking about the covariance matrix a lot lately. An estimate for any given set of continuous variables can be obtained using the `cov` function. You can see that the diagonals of this matrix are the same as the variances calculated one at a time from before.

```{r}
# Create a covariance matrix
cov(reduced)

# Another way to get total variance
sum(diag(cov(reduced)))
```

Running PCA is relatively straightforward. The following script conducts a PCA using the covariance matrix (nonstandardized variables) and stores the results in an object. This object contains the eigenvectors, eigenvalue, and the new principle component vectors. Lets start by producing a correlation matrix to verify that new principle component variables are uncorrelated.

```{r}
# Run principle component analysis on reduced data
pc_result <- prcomp(reduced, scale. = FALSE)

# View results of the PCA
pc_result

# Get scores
pc_scores <- pc_result$x

# Pairs plots of PC Scores
pairs(pc_scores)

# Correlation of PC Scores
cor(pc_scores)
```

We can again verify that the total variance in the new PC variables is exactly the same as the original data. The eigenvectors are stored inside of "pc_result" as well in the "rotation" object.

```{r}
var_pca <- apply(pc_scores, 2, var)
var_pca

# Total Variance of PC's
sum(var_pca)

# Total Variance of Original Variables
sum(apply(reduced, 2, var))

# List of eigenvectors
pc_result$rotation
```

A scree plot of the eigenvalues used to determine how many PC's to keep can be plotted in the following way:

```{r}
par(mfrow = c(1, 2))
eigenvals <- (pc_result$sdev)^2
plot(1:5, eigenvals / sum(eigenvals), type = "l", main = "Scree Plot", ylab = "Prop. Var. Explained")
cumulative.prop <- cumsum(eigenvals / sum(eigenvals))
plot(1:5, cumulative.prop, type = "l", main = "Cumulative proportion", ylim = c(0, 1))
par(mfrow = c(1, 1))
```

Since all of the variables are not on the same scale, we see a very similar phenomenon that we discussed in the pre-live session. To conduct the PCA on the correlation matrix, just set `scale. = TRUE` inside of the `prcomp` function.

## HW Assignment #1

1. Conduct the PCA analysis but use the entire set of variables starting with column 6, the Games played variable, all the way down to the end at GIDP. Provide a scree plot and determine the amount of PC's needed to retain approximately 90% of the total variation in the data set.

```{r}
# Run PCA
hw_q1_1 <- princomp(bat_16[, 6:22], scores = TRUE)

# Scree Plot
screeplot(hw_q1_1)                  # bars
screeplot(hw_q1_1, type = "lines")  # lines
```

**It looks as though only one PC is needed to retain approximately 90% of the total variation in the data set.**

2. Provide the eigenvector matrix and examine the loading (coefficients) that determine the linear combinations of each principle component. Verify that PC1 is essentially a weighted average of all the variables together (minus the SH, sacrifice hit variable.) 

```{r}
# Summary Stats
# summary(hw_q1_1)
hw_q1_1$loadings
# hw_q1_1$scores
```

3. Verify that PC2 has big negative loadings on triples (X3B), stolen bases (SB), caught stealing (CS), and sacrifice hits (SH). This variable could be interpreted to be a general indication of a players speed or general utility since all of the variables require situation awareness and running ability. (You don't need to provide an answer here, just verify))

## PCA as an Exploratory Technique for Classification

This exercise is designed to walk you through how PCA can be used as an informative unsupervised analysis of your predictors to get a high level view of whether the predictors are actually going to do a good job or not before a predictive model for categorical responses is even applied.

The following data set is a breast cancer data set that has numerous measurements taken from tumor biopsies. The goal of using this data set is to predict using the metrics alone if the biopsy is cancer or not. When continuous variables are available it is often helpful to create a pairs plot of data color coded by the response status (Diagnosis). The first variable is an id number and is not needed.

```{r message=FALSE, warning=FALSE}
bc <-
 read.table(
  "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",
  header = F,
  sep = ","
 )
names(bc) <- c(
 "id_number",
 "diagnosis",
 "radius_mean",
 "texture_mean",
 "perimeter_mean",
 "area_mean",
 "smoothness_mean",
 "compactness_mean",
 "concavity_mean",
 "concave_points_mean",
 "symmetry_mean",
 "fractal_dimension_mean",
 "radius_se",
 "texture_se",
 "perimeter_se",
 "area_se",
 "smoothness_se",
 "compactness_se",
 "concavity_se",
 "concave_points_se",
 "symmetry_se",
 "fractal_dimension_se",
 "radius_worst",
 "texture_worst",
 "perimeter_worst",
 "area_worst",
 "smoothness_worst",
 "compactness_worst",
 "concavity_worst",
 "concave_points_worst",
 "symmetry_worst",
 "fractal_dimension_worst"
)

# Getting a look at the distribution
table(bc$diagnosis)

# Scatter plots color coded by response for just the first few variables
bc %>% 
  select(2:6) %>% 
  group_by(diagnosis) %>% 
  ggpairs(aes(color = diagnosis)) +
  theme_minimal()
  
pairs(bc[, 3:6], col = as_factor(bc$diagnosis))
```

So we can see from this pairs plot of just the first few variables, separation between the cancer and non cancer groups are pretty well separated. Unfortunately we may not always see clear separations but that does not necessarily mean that something like LDA or some other predictive tool won't work. It could be due to the fact we can't see the separation of the groups unless we can actually see in higher dimensions. One way to still get at this is to conduct a PCA analysis and provide a some scatter plots for the first few PC's. If separation exists in the PC's, then a predictive model will probably do well.

Below we will conduct PCA on all of the predictors and plot the first few PC's against each other and look for separation. The number of PCs to explore can be dictated by the scree plot.

```{r}
pc_bc <- prcomp(bc[,-c(1, 2)], scale. = TRUE)
pc_bc_scores <- pc_bc$x

# Adding the response column to the PC's data frame
pc_bc_scores <- as_tibble(pc_bc_scores)
pc_bc_scores$Diagnosis <- bc$diagnosis

# Use ggplot2 to plot the first few PC's
ggplot(data = pc_bc_scores, aes(x = PC1, y = PC2)) +
 geom_point(aes(col = Diagnosis), size = 3, alpha = 0.5) +
 labs(title = "PCA of Breast Cancer Tumor Biopsies") +
  theme_minimal()+
  NULL

ggplot(data = pc_bc_scores, aes(x = PC2, y = PC3)) +
  geom_point(aes(col = Diagnosis), size = 3, alpha = 0.5) +
 labs(title = "PCA of Breast Cancer Tumor Biopsies") +
  theme_minimal() +
  NULL
```

So we can see in the first graphic a clear separation exists for the two cancer groups. So the PCA is telling us in effect what we already know from looking at the original variables. The power of this approach is that you only need to look at 2-4 graphs each time, versus potentially having to examine massive scatter plot matrices to see if anything is there or not!

## HW Assignment 2

1. Given what we see in the PCA analysis, it's not too surprising that an LDA will probably do a good job here in predicting the categorical responses. Perform an LDA on the original set of variables and calculate a confusion matrix. Note: For this problem you do not have to do a training and test set split, let's recognize that the prediction performance that we obtain is potentially biased too low due to overfitting. The main point here is that the accuracy is pretty good as expected via the PCA look.

```{r}
library(MASS)

# Run LDA
bc_lda <- lda(diagnosis ~ ., data = bc[, -1], CV = TRUE)

# Create confusion matrix
tab <- table(bc$diagnosis, bc_lda$class)
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
# dimnames(conCV1) <- list(Actual = c("No", "Yes"), "Predicted (cv)" = c("No", + "Yes"))
print(round(conCV1, 3))
```

2. Consider now another great sanity check when building predictive models. The code below takes the original data set and randomly scrambles the response variable. This effectively breaks up any relationship that existed between the predictors and the response. 

```{r}
fake <- bc
fake$diagnosis <- sample(fake$diagnosis, 569, replace = F)
```

 a. Plot PC1 and PC2 using the scrambled data set. 

```{r}
# Run PCA
pc_fake <- prcomp(fake[,-c(1, 2)], scale. = TRUE)
pc_fake_scores <- pc_fake$x

# Adding the response column to the PC's data frame
pc_fake_scores <- as_tibble(pc_fake_scores)
pc_fake_scores$Diagnosis <- fake$diagnosis

# Use ggplot2 to plot the first few PC's
ggplot(data = pc_fake_scores, aes(x = PC1, y = PC2)) +
 geom_point(aes(col = Diagnosis), size = 3, alpha = 0.5) +
 labs(title = "PCA of Breast Cancer Tumor Biopsies") +
  theme_minimal()+
  NULL

ggplot(data = pc_fake_scores, aes(x = PC2, y = PC3)) +
  geom_point(aes(col = Diagnosis), size = 3, alpha = 0.5) +
 labs(title = "PCA of Breast Cancer Tumor Biopsies") +
  theme_minimal() +
  NULL
``` 

 b. Perform an LDA with this data set and look at the confusion matrix. Do they correspond?
 
Note: This little trick is extremely helpful when you are predicting a response that is heavily imbalanced (ex: lots of Cancer obs, few Healthy ones ). LDA and other algorithms can behave quite weirdly in extreme cases and prediction performances can look good all the time. By conducting a separate analysis on scrambled data, if the prediction performance still looks good, you've recognized a problem. We can discuss this topic more as we get closer to finishing up Project 2.

```{r}
library(MASS)

# Run LDA
fake_lda <- lda(diagnosis ~ ., data = fake[, -1], CV = TRUE)

# Create confusion matrix
tab <- table(fake$diagnosis, fake_lda$class)
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
# dimnames(conCV1) <- list(Actual = c("No", "Yes"), "Predicted (cv)" = c("No", + "Yes"))
print(round(conCV1, 3))
```

