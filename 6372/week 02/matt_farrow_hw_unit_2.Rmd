---
title: "6372: Unit 2 Homework"
author: "Matt Farrow"
date: ''
output:
 html_document:
  df_print: paged
 word_document: default
fig_caption: yes
editor_options: 
 chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## HW Instructions

The weekly HW assignments are designed to accomplish 2 goals for the MSDS student. The first is to provide a series of conceptual and analtical questions so the student can get a feel for their current understanding of the unit. The second goal is to introduce the students to standard functions and routines in R that effectively do the same things that the "Procs" do in SAS.

R and SAS are both wonderful tools and as we go through the assignments, students will begin to recognize very quickly that they both have pros and cons. 

The formatting of the HW is as follows: 
 
 1. A series of high level questions will be asked with either short answers or simple multiple choice responses.
 
 2. Analytical questions will be provided but a short vignette example of how R functions work for a given topic or method will be given. The student will then be asked a follow up question or two based on the output provided. 
 
 3.Thirdly, a new data set will be given to allow the student to gain some experience with a new data set from start to finish. This part of the homework is for your own practice and is not due at the time of HW submission.
 
Solutions to the HW will be provided a day or two after the HW is submitted. It is up to the student to "shore up" any confusion or missunderstanding of a topic. Grading will be based on a combination of correctness, completion, and overall conciseness. 

## MLR Conceptual questions

 1. State the necessary assumptions for a multiple linear regression model to be valid in terms of conducting hypothesis tests and providing prediction intervals. 
  - **The assumptions for a multiple linear regression model to be valid include: residuals that are normally distributed, constant variance, and independent observations.**
 2. True or False?  It is required that the response variable of an MLR model be normally distributed. 
  - **True**
 3. Feature selection such as forward, backward, stepwise, or penalized techniques provide analysts the ability to do...what? For MLR, what can a feature selection method not do without the guidance of the analyst? 
  - **Feature selection allows analysts to determine which predictors are relevant in a regression model. One thing that a feature selection method not do with out the guidance of the analyst is perform transformations on the variables being considered.**
 4. Supose that the the true relationship between a response variable ,$y$, and explanatory variable $x$ is $y=\beta_0+\beta_1x+\beta_2x^2+\epsilon$. If the analysts final model includes a third, fourth, and fifth polynomial term, state whether the test ASE will be higher or lower than the training ASE. Is this an example of a model having high bias or high variance? 
  - **In this example, if additional polynomial terms are added, the ASE will be higher than the training ASE. This is an example of a model having high variance.**
 
## Exercise #1 EDA 

The first step of any model building process is to conduct a thorough exploratory data analysis (EDA) once the main study question and goals are defined. To motivate this topic using R we will use a widely known data set involving modeling miles per gallon (mpg) of cars with numerous makes and models. The data set is available in the package "ISLR".

Let's view the data first below. The goal of this study is to understand what explanatory variables contribute to the variation in the mpg values. Among the variables in the data set, we have information such as how many cylinders the vehicle has, year it was made, horsepower, displacement, acceleration, and origin. Cylinders and origin are categorical variables. Origin is the only one that is a little cryptic. There are 3 categorical levels: 1-US, 2-Europe, and 3-Japan/Asia.

```{r mpg}
library(ISLR)
head(Auto)
```

When categorical variables have levels that are coded as numeric it is important to make sure that R is treating them as categorical. The following code ensures that origin and cylinders are indeed categorical and then the "attach" function allows us to call the variable names in the data set Auto without having to use the dollar sign convention.

```{r clean}
Auto$cylinders <- as.factor(Auto$cylinders)
Auto$origin <- as.factor(Auto$origin)
attach(Auto)
```

To better understand what variables we have access to, some simple summary statistics for each variable are helpful for identifying any obvious outliers that could be recording errors or just simply understanding the range of each predictor so we do not extrapolate any predictions if that is something we are asked to do later.

A quick and dirty summary is to just use summary.

```{r sumtab}
summary(Auto)
```

For categorical explanatory variables it can be helpful to view summary statistics by each categorical level. Examining cylinders below we can quantify the shifts a little better using means. Note we could swap out the "summary" function for any other function we would want to statistics on.

```{r}
# Summary statistics by categorical level
t(aggregate(mpg ~ cylinders, data = Auto, summary))

# Means of each cylinder
t(aggregate(mpg~cylinders,data=Auto,mean))

# Standard deviations of each cylinder
t(aggregate(mpg~cylinders,data=Auto,sd))
```

From the summary statistics, we can see that origin and cylinders are treated as categorical. It's interesting to note that there are only a few cars that have 3 and 5 cylinders. For building a model for interpretation this is probably fine as is, however if we are doing something a little more complex and utilizing cross validation or training and test set splits, not all categorical levels may be available in the training set so removing of those rare cars may be worth doing. For now we will keep them.

It is also useful to explore how the potential explanatory variables may be correlated to the response.

```{r visual1, fig.height=2}
par(mfrow = c(1, 3))
plot(horsepower, mpg, xlab = "horsepower", ylab = "mpg")
new <- data.frame(horsepower = seq(30, 300, .1))
lines(seq(30, 300, .1),
   predict(lm(mpg ~ horsepower), newdata = new),
   col = "red",
   lwd = 4)

plot(
 as.factor(cylinders),
 mpg,
 xlab = "cylinders",
 ylab = "mpg",
 title = "Auto Data Set",
 col = c(7, 32, 57, 82, 107)
)

plot(weight, mpg)
new2 <- data.frame(weight = seq(1600, 5200, 1))
lines(seq(1600, 5200, 1),
   predict(lm(mpg ~ weight), newdata = new2),
   col = "red",
   lwd = 4)
```

There are a couple of things to note in the figures above. All three explanatory variables seem to be trending with the mpg. The more cylinders you have tends to decrease mpg (except for 3). There are also downward trends with respect to horsepower and weight which all makes sense. They appear to be nonlinear so that may be something to consider later during the modeling process. Additional graphics could be made for the other variables as well to explore.
 
For interpretation of MLR models, it is necessary to examine the issues of multicollinearity. If the explanatory variables are highly associated with each other, regression coefficients can become unreliable and provide inconsistent interpretations. The following code provides the means to make pairwise scatter plots. It is also helpful to examine these plots while color coding for additional categorical explanatory variables.  
 
```{r}
pairs(Auto[, -c(2, 9)])
```
 
The scatter plot matrix above lets us examine the relationship each predictor has with the response (top row) and the remaining plots allow us to assess multicollinearity. If we color code the scatterplot matrix additional insight could be gained. Below is a pairs plot color coded by cylinder. Here we could visually assess if the relationships we see may depend on what cylinder group the observation belongs too. If seperate lines with different slopes for each group looks like a better fit, then an interaction could be included to try to capture that level of complexity.

```{r}
pairs(Auto[, -c(2, 9)], col = cylinders)
```

To obtain the VIFs of each explanatory variable, we just need to run a sigle MLR fit using all the predictors and then make a vif call. *Note: in practice you can manually make decisions to remove redundant variables. You don't need to live and die by VIF assessments.* The code below provides a clever syntax trick. If your data set has nothing but the response and explanatory variables contained in the data frame and you wish to run a model using all of the predictors then you can use the following shorthand syntax.

```{r}
library(car) # where vif function lives
Auto <- Auto[, -9] # removing the car name
full.model <- lm(mpg ~ ., data = Auto) # . means all variable not mpg
vif(full.model)[, 3]^2
```

### Homework Question 5

Use this section's EDA output to provide some comments on whether or not multicollinearity may be an issue for this data set. If so, what variables do you think are redundant and could be removed? You do not need to do any additional work, but you are welcome to if you want. 

**Based on the scatterplots, it appears that weight and displacement may be suffering from multicollinearity. A look at the VIF values indicates that `displacement`, `horsepower`, and `weight` all have values over 10, although `horsepower` is just above that threshold (10.559221).**

## Model Assessment and Modeling Potential Interaction/Model Complexity

Recall from the exploratory work, it looked like horsepower was associated with mpg but it might be a little nonlinear. Let's suppose for a moment that the only available predictor to us is horsepower and cylinder. How could we tell if additional complexity is needed to model the apparent nonlinear trend? Is there any other way that nonlinear trend could be explained by a MLR model? Let's take a look. Below is a simple linear regression fit of MPG on horsepower. The two right plots are residuals versus fitted and residuals vs horsepower. 

```{r , fig.height=2}
par(mfrow = c(1, 3))
plot(horsepower, mpg, xlab = "horsepower", ylab = "mpg")
new <- data.frame(horsepower = seq(30, 300, .1))
horse.model <- lm(mpg ~ horsepower)
lines(seq(30, 300, .1),
   predict(horse.model, newdata = new),
   col = "red",
   lwd = 4)
plot(
 horse.model$fitted.values,
 horse.model$residuals,
 xlab = "Fitted Values",
 ylab = "Residuals"
)
plot(horsepower,
   horse.model$residuals,
   xlab = "Horsepower",
   ylab = "Residuals")
```

There are two noticable things going on the residual plots. One we can see as horsepower increases the variability about the trend line increases (non-constant variance). Secondly, there is a clear trend in the residuals in which it doesn't appear to be centered at 0 all the way through. This indicates that the model is not complicated enough and are predictions are biased in some areas.

One obvious approach is to include a quadratic term and follow up with some additional diagnostic looks. You can see below that the quadratic component eliminates the trending behavior in the diagnostics but constant variance is still an issue. A remedy for that is log transforming mpg.

```{r, fig.height=2}
par(mfrow = c(1, 3))
plot(horsepower, mpg, xlab = "horsepower", ylab = "mpg")
new <- data.frame(horsepower = seq(30, 300, .1))
horse.model2 <- lm(mpg ~ horsepower + I(horsepower ^ 2))
lines(seq(30, 300, .1),
   predict(horse.model2, newdata = new),
   col = "red",
   lwd = 4)
plot(
 horse.model2$fitted.values,
 horse.model2$residuals,
 xlab = "Fitted Values",
 ylab = "Residuals"
)
plot(horsepower,
   horse.model2$residuals,
   xlab = "Horsepower",
   ylab = "Residuals")
```

```{r, fig.height=2}
par(mfrow = c(1, 3))
plot(horsepower,
   mpg,
   xlab = "horsepower",
   ylab = "mpg",
   col = cylinders)
new <- data.frame(horsepower = seq(30, 300, .1))
horse.model2 <- lm(mpg ~ horsepower + I(horsepower ^ 2))
lines(seq(30, 300, .1),
   predict(horse.model2, newdata = new),
   col = "red",
   lwd = 4)
plot(
 horse.model2$fitted.values,
 horse.model2$residuals,
 xlab = "Fitted Values",
 ylab = "Residuals"
)
plot(horsepower,
   horse.model2$residuals,
   xlab = "Horsepower",
   ylab = "Residuals")
```

In general for any regression model you fit in R, the user can obtain basic residual diagnostic using the plot option. There are four plots total.

```{r,fig.height=5,fig.width=5}
par(mfrow = c(2, 2))
plot(horse.model2)
```

### Homework Question 6

Reanalyze the regression model (produce the same 3 figures from above) using horsepower (and its quadratic term) alone but log transforming mpg. Does the transformation fix the constant variance issue? How does normality of the residuals look? The plot of log(mpg) vs horsepower looks like the trend may be more linear now and a quadratic term is not needed? Use residual diagnostics and the summary function (summary(yourmodel)) to determine if a quadratic term is needed after log transforming. 

```{r, fig.height=2}
par(mfrow = c(1, 3))
plot(horsepower,
   log(mpg),
   xlab = "horsepower",
   ylab = "mpg",
   col = cylinders)
new <- data.frame(horsepower = seq(30, 300, .1))
horse.model2 <- lm(log(mpg) ~ horsepower + I(horsepower ^ 2))
lines(seq(30, 300, .1),
   predict(horse.model2, newdata = new),
   col = "red",
   lwd = 4)
plot(
 horse.model2$fitted.values,
 horse.model2$residuals,
 xlab = "Fitted Values",
 ylab = "Residuals"
)
plot(horsepower,
   horse.model2$residuals,
   xlab = "Horsepower",
   ylab = "Residuals")
```

**The log transformation of mpg does appear to have fixed the constant variance issue, and the quadratic term may no longer be needed. The residuals appear to be a more normally distributed cloud of points as well which speaks well of the log transformation.**

```{r}
summary(horse.model2)
```

## Feature Selection and the Bias-Variance Trade Off

As discussed during the live session, feature selection is a helpful tool to safeguard our model building processes from building models that are potentially too simple or overly complex. To get a sense of how to replicate tools we have seen in SAS, lets import the golf data set that we used in our prelive assignment and play around. (NOTE: There are more integrated R pacakges for example caret that incorporate other feature selection technques and predictive models. More integrated often translates to more "black box" and I want to make sure we have a good sense of this critical concept.) 

The leaps packages has a very straightforward tool to use for forward selection. Regsubsets basically works like a general lm regression call with the added options of selecting the type of selection process and the maximum number of steps you want to go.

```{r echo=T, fig.keep = 'none'}
library(leaps)
golf <- read.csv(here::here("6372", "week 02", "pre-live", "GolfData2.csv"), header = T)
golf <- golf[, -c(1, 8, 9, 10)]
reg.fwd <- regsubsets(log(AvgWinnings) ~ ., data = golf, method = "forward", nvmax = 20)
```

The object that I created, reg.model, contains the results of not only the final regression model but all of the models through the iterative process. This allows us to make graphics similar to the SAS diagnostics of glmselect.

Using the summary command we can extract metrics like ASE, AIC, and BIC for each of the steps. The vector of results are listed in order of model complexity (1 predictor up to nvmax). Some examples are below along with a simple visualization which is helpful.Keep in mind at this point, we are not doing any CV or even training and test set splits. We just using the entire data set as a training set.

```{r, echo=T, fig.height=3,fig.width=7}
summary(reg.fwd)$adjr2
summary(reg.fwd)$rss
summary(reg.fwd)$bic

par(mfrow = c(1, 3))
bics <- summary(reg.fwd)$bic
plot(1:20, bics, type = "l", ylab = "BIC", xlab = "# of predictors")
index <- which(bics == min(bics))
points(index, bics[index], col = "red", pch = 10)

adjr2 <- summary(reg.fwd)$adjr2
plot(1:20, adjr2, type = "l", ylab = "Adjusted R-squared", xlab = "# of predictors")
index <- which(adjr2 == max(adjr2))
points(index, adjr2[index], col = "red", pch = 10)

rss <- summary(reg.fwd)$rss
plot(1:20, rss, type = "l", ylab = "train RSS", xlab = "# of predictors")
index <- which(rss == min(rss))
points(index, rss[index], col = "red", pch = 10)
```

When dealing with only a training data set metrics like AIC and BIC are the most appropriate as they penelize the more complex models. (See the BIC graph). Now lets visit the train/test set split for checking model adequacy.

First lets simply split the golf data set into training and split and fit a forward selection model on the training data set.

```{r,echo=T}
set.seed(1234)
index <- sample(1:dim(golf)[1], 100, replace = F)
train <- golf[index, ]
test <- golf[-index, ]
reg.fwd <- regsubsets(log(AvgWinnings) ~ ., data = train, method = "forward", nvmax = 20)
```

Once we have our model fits on the training data set, all we need to do is predict the models onto the training data set and produce ASE type plots for each step of the forward selection. Courtesy of the ISLR texbook, a function is provided to easily predict the forward selection results on a test set.

```{r, echo=T}
# Really handy predict function
predict.regsubsets <- function(object, newdata, id, ...) {
 form <- as.formula(object$call [[2]])
 mat <- model.matrix(form, newdata)
 coefi <- coef(object, id = id)
 xvars <- names(coefi)
 mat[, xvars] %*% coefi
}
```

With the prediction function read in we can simply write a loop to predicted on each of the 20 models generated from the forward selection procedure and plot the ASE's. I've included the training ASE for comparison.

```{r, echo=T, fig.width=5,fig.height=4}
testASE <- c()
# note my index is to 20 since that what I set it in regsubsets
for (i in 1:20) {
 predictions <- predict.regsubsets(object = reg.fwd, newdata = test, id = i)
 testASE[i] <- mean((log(test$AvgWinnings) - predictions)^2)
}
par(mfrow = c(1, 1))
plot(1:20, testASE, type = "l", xlab = "# of predictors", ylab = "test vs train ASE", ylim = c(0.3, 0.8))
index <- which(testASE == min(testASE))
points(index, testASE[index], col = "red", pch = 10)
rss <- summary(reg.fwd)$rss
lines(1:20, rss / 100, lty = 3, col = "blue") # Dividing by 100 since ASE=RSS/sample size
```

From the test ASE graphic, we see (via the red dot) that the minimum Average Squared Error happens with 3 predictors included in the model and its value is around 0.4. The ASE on the test gives us a metric on how reproducible the models being fitted would be have on a future data set. It doesn't really help us get at how well the prediction accuracy is though. To finish things off and after additional model fitting and trial and error, once a model is deemed the final model it is usually standard practice to fit the entire data set once more and make your final reports and conclusions using all of the data (more information to estimate parameters).

```{r, echo=T}
reg.final <- regsubsets(log(AvgWinnings) ~ ., data = golf, method = "forward", nvmax = 4)
coef(reg.final, 3)
final.model <- lm(log(AvgWinnings) ~ Greens + AvgPutts + Save, data = golf)
summary(final.model)
```

Remember that the output of the final model in terms of reporting p-values and interpretation, you still must check the model assumptions to make sure everything is valid. As mentioned earlier the ASE plots are really great at helping to select a final model that doesn't have too much bias or variance, it is hard to tell how well the predictions are. This depends on the scale of the response variable. A helpful graphic that SAS produces as well is the true response versus the predictions. This can give you a sense of the uncertainty in the prediction. More variability equates to more uncertainty.

```{r,echo=T}
plot(exp(final.model$fitted.values), golf$AvgWinnings, xlab = "Predicted", ylab = "AvgWinnings", xlim = c(0, 400000), ylim = c(0, 400000))
lines(c(0, 400000), c(0, 400000), col = "red")
```

Another helpful thing that this graph shows that also would come up in the residual diagnostics is that our predictive modelis going to under predict those extreme golfers who are making more than everybody else. We would have need some other explantory variable to help capture what is going on out there.

To help with the idea of this graph imagine we fit another predictive model but just used 3 of the random variables. This should clearly have less predictive ability and we could verify that with a test ASE assessment. Below is the predictions of the "bogus" model versus the true Avg Winnnings. We can see the predictions are much worse as they vary much more around the red line.

```{r, echo=F}
bogus.model <- lm(log(AvgWinnings) ~ V12 + V13 + V14, data = golf)
plot(exp(bogus.model$fitted.values), golf$AvgWinnings, xlab = "Predicted", ylab = "AvgWinnings", xlim = c(0, 400000), ylim = c(0, 400000))
lines(c(0, 400000), c(0, 400000), col = "red")
```

Probably the best way is to make a few predictions and examine and take a look at their prediction intervals. The tighter the interval the better model you have and you can make some practical sense from there. Another helpful thing to do would be to take a look at some prediction intervals. These are on the log scale. 

```{r, echo=T}
head(predict(final.model, golf, interval = "predict"))
```

Putting things back on the raw scale, we can see the certainty in our predictions.
For example, the first predicted average winnings for a golfer with Green=58.2, AvgPutts=1.767, and Save=50.9 is $11994.16 with a prediction interval of 3247.77 to 44,294. The interval is quite large and illustrates just how variable average winnings are even though there are a few key predictors that are statistically relevent and the model producing results that make some sense.

### Homework Question 7

Use the Auto data set and perform the following tasks. Provide your R script for these answers.

1. Split the data set into a training and test set that is roughly 50/50. Before doing so delete the observations that have a 3 or 5 cylinders. This will make your life easier. To keep everyone consistent with each other, make sure you set the seed number "set.seed(1234)" before making the split. 

```{r echo=FALSE}
library(dplyr)
```

```{r}
set.seed(1234)

# Create data set
df <- Auto %>% 
  filter(!cylinders %in% c(3, 5))

# Build test and training data sets
df_index <- sample(1:dim(df)[1], 192, replace = F)
df_train <- df[df_index, ]
df_test <- df[-df_index, ]
```

2. By choosing to delete the 3 and 5 cylinder cars from the model what does that change, if anything, about the conclusions one could make from an analysis conducted from this new reduced data set?

**The only conclusions we could draw from the data set would relate to cars with 4, 6, or 8 cylinders since 3- and 5-cylinder cars were excluded.**

3. Perform a forward selection on the training data set and produce the ASE plot comparing both the training and test set ASE metrics using the following set of predictions:  

  - `displacement`
  - `horsepower`
  - `weight`
  - `acceleration`
  - `year`
  - `origin`

  Determine how many predictors should be included. Set the nvmax to 7 for this problem.

```{r}
# Run forward selection
df_fwd <-
  regsubsets(cylinders ~ .,
             data = df_train,
             method = "forward",
             nvmax = 7)

# Really handy predict function
predict_regsubsets <- function(object, newdata, id, ...) {
 form <- as.formula(object$call [[2]])
 mat <- model.matrix(form, newdata)
 coefi <- coef(object, id = id)
 xvars <- names(coefi)
 mat[, xvars] %*% coefi
}

# Create empty test ASE
testASE <- c()

# For loop
for (i in 1:7) {
  predictions <-
    predict.regsubsets(object = df_fwd,
                       newdata = df_test,
                       id = i)
  testASE[i] <- mean((as.numeric(df_test$cylinders) - predictions) ^ 2)
}

par(mfrow = c(1, 1))
plot(
  1:7,
  testASE,
  type = "l",
  xlab = "# of predictors",
  ylab = "test vs train ASE",
)
index <- which(testASE == min(testASE))
points(index, testASE[index], col = "red", pch = 10)
rss <- summary(df_fwd)$rss
lines(1:7, rss / 192, lty = 3, col = "blue")
```

4. Using your decision from #3, fit a final model using the entire data set and produce the residual diagnostics. Does your model look reasonable or do you think you should consider some additional iterations? Give a brief comment. You do not need to act on the comments.

```{r}
reg_final <- regsubsets(cylinders ~ ., data = df, method = "forward", nvmax = 7)
coef(reg_final, 6)
final_model <- lm(cylinders ~ mpg + displacement + horsepower + weight + year + origin, data = df)
summary(final.model)
```

5. *(optional)* What happens when you try to do forward selection using all of the variables in the data set? Can you potentially give some reasons why warnings/errors start popping up when doing so? Hint: Using table(train$cylinder,train$origin) maybe helpful among other simple summary stats and graphics.  

## LASSO Calls

There are no homework questions involving this section but it is here for the sake of your project. Peforming a lasos call is pretty straight forward. GLM-NET peforms 10-fold CV to determine an optimal penalty parameter. The coefficients are easy to extract and making predictions are straight forward. It is possible to make ASE style plots like we did previously but it takes a little extra programming. See me if interested.

```{r,echo=T}
library(glmnet)

# Formatting data for GLM net
x <- model.matrix(AvgWinnings ~ ., train)[, -1]
y <- log(train$AvgWinnings)

xtest <- model.matrix(AvgWinnings ~ ., test)[, -1]
ytest <- log(test$AvgWinnings)

grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x, y, alpha = 1, lambda = grid)

cv.out <- cv.glmnet(x, y, alpha = 1) # alpha=1 performs LASSO
plot(cv.out)
bestlambda <- cv.out$lambda.min # Optimal penalty parameter. You can make this call visually.
lasso.pred <- predict(lasso.mod, s = bestlambda, newx = xtest)

testMSE_LASSO <- mean((ytest - lasso.pred)^2)
testMSE_LASSO
```

Note here that the ASE (MSE) on the test set is very similar to the forward selection results previously examined. The number of predictors in this LASSO fit is larger than what forward selection, but examining the coefficients on the predictors we can see that many of them are quite small and not really contributing much.

```{r, echo=T}
coef(lasso.mod, s = bestlambda)
```
