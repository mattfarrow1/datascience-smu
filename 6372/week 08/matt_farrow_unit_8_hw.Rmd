---
title: '6372: Unit 8 Homework'
author: "Matt Farrow"
date: ''
output:
 html_document:
 df_print: paged
 word_document: default
 pdf_document: default
fig_caption: yes
editor_options:
 chunk_output_type: console
---

## MANOVA/LDA Conceptual Questions

1. State the assumptions of the MANOVA and LDA models.  

  - Residuals are multivariate normal (MVNORM) with a constant covariance matrix

2. What is the fundamental difference between MANOVA and LDA since they both share the same underlying assumptions?

  - MANOVA compares two or more categorical variables across a range of continuous variables.
  - LDA focuses on prediction of new observations into the existing categorical variables. 
  
3. What is a confusion matrix? 

  - A confusion matrix is a table that compares actual values with predicted values to determine a model's accuracy.

4. What is the difference between LDA and QDA, that is, what assumption of the LDA model is relaxed?

  - QDA is a more flexible classifier than LDA because it relaxes the assumption that the two distributions have equal variance.

## Exercise 1: Visualizing LDA and QDA

As discussed in live session, LDA has some nice geometrical representations that can be really helpful. The examples for this exercise are simulated to illustrate the point and facilitate learning. This is typically not done in practice unless you are working in the special case of using exactly 2 predictors. 

Let's consider a simple example with two predictors trying to predict a response with two categories (Yes and No). However, only one of the predictors, "X2" is really helpful in predicting. The following script simulates a data set and generates a visualization so we can see what we have. I encourage you to take a quick view of the data set "Full" so you can see the structure of the data set in which LDA will be applied later. 

```{r echo=TRUE, fig.height=3.5}
library(MASS)
library(mvtnorm)
set.seed(1234)

# Create yes and no vectors
dataYes <- mvrnorm(30, c(10, 10), matrix(c(1, .8, .8, 1), 2, 2, byrow = T))
dataNo <- mvrnorm(30, c(10, 7), matrix(c(1, .8, .8, 1), 2, 2, byrow = T))

# Bind yes and no together into a data frame
full <- data.frame(rbind(dataYes, dataNo))

# Add yes/no column as a factor
full$Response <- factor(rep(c("Yes", "No"), each = 30))

# Plot data
plot(full[, 1:2], col = full$Response, main = "Shift in X2")
```

#### My `tidyverse` recreation:

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.height=3.5}
library(tidyverse)

# Create data set
full2 <- bind_rows(as_tibble(mvrnorm(30, c(10, 10), matrix(c(1, .8, .8, 1), 2, 2, byrow = T))),
                  as_tibble(mvrnorm(30, c(10, 7), matrix(c(1, .8, .8, 1), 2, 2, byrow = T))))

# Create response column
full2 <- full2 %>% 
  mutate(response = as_factor(rep(c("Yes", "No"), each = 30)))

# Plot results
ggplot(full2, aes(V1, V2, color = response)) +
  geom_point(shape = 21) +
  labs(title = "Shift in V2",
       color = "Response") +
  theme_minimal()
```

You can see from the graphic above that the predictor X2 is really what's important here as the two clouds of points are essentially the same, just shifted upward. As discussed during live session, what we need for LDA is for the point clouds for the two response categories to have similar elliptical shapes. Gross departures from this would require the use of QDA instead.

Building an LDA model is probably the easiest of all the prediction models to run syntactically. The first line of the following step is all that is needed to fit the model. The remaining code that produces the prediction boundary of the LDA on the scatter plot isn't used in practice, but is helpful for this example to see what LDA is doing. The two plus signs added to the figure are just the sample means of the predictors for each response group.

```{r}
# Construct the LDA model
mylda <- lda(Response ~ X1 + X2, data = full)

# Draw discrimination line
np <- 300
nd.x <- seq(
  from = min(full$X1),
  to = max(full$X1),
  length.out = np
)
nd.y <- seq(
  from = min(full$X2),
  to = max(full$X2),
  length.out = np
)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)

prd <- as.numeric(predict(mylda, newdata = nd)$class)

plot(full[, 1:2], col = full$Response, main = "Shift in X2")

points(
  mylda$means,
  pch = "+",
  cex = 2,
  col = c("black", "red")
)

contour(
  x = nd.x,
  y = nd.y,
  z = matrix(prd, nrow = np, ncol = np),
  levels = c(1, 2),
  add = TRUE,
  drawlabels = FALSE
)
```

### Question 5

Consider the following simulated data set where both predictors are now helpful in predicting the response. Rinse and repeat the previous code and produce a plot with the LDA prediction boundary to see how things have changed from our first scenario. As long as you store your LDA model in an object called "mylda", you should not have to modify any of the script to obtain the prediction boundary plot.

```{r, echo=T}
# Another scenario
dataYes <- mvrnorm(30, c(10, 10), matrix(c(1, .6, .6, 1), 2, 2, byrow = T))
dataNo <- mvrnorm(30, c(8, 8), matrix(c(1, .6, .6, 1), 2, 2, byrow = T))
full <- rbind(dataYes, dataNo)
full <- data.frame(full)
full$Response <- rep(c("Yes", "No"), each = 30)
full$Response <- factor(full$Response)
names(full)[1:2] <- c("X1", "X2")
```

```{r}
# Construct the LDA model
mylda <- lda(Response ~ X1 + X2, data = full)

# Draw discrimination line
np <- 300
nd.x <- seq(
  from = min(full$X1),
  to = max(full$X1),
  length.out = np
)
nd.y <- seq(
  from = min(full$X2),
  to = max(full$X2),
  length.out = np
)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)

prd <- as.numeric(predict(mylda, newdata = nd)$class)

plot(full[, 1:2], col = full$Response, main = "Shift in X2")

points(
  mylda$means,
  pch = "+",
  cex = 2,
  col = c("black", "red")
)

contour(
  x = nd.x,
  y = nd.y,
  z = matrix(prd, nrow = np, ncol = np),
  levels = c(1, 2),
  add = TRUE,
  drawlabels = FALSE
)
```

### Question 6

One of the options of LDA is to incorporate prior information. Use the ?lda for details on the prior option. Suppose now that for new data sets coming in that we wish to predict on, we know that 80% of the time they are going to be a "Yes" outcome. Incorporate the prior information and examine how the prediction boundary of LDA has changed.

```{r}
# Construct the LDA model
mylda <- lda(Response ~ X1 + X2, 
             # prior = c(0.8, 0.2), 
             prior = c(0.2, 0.8), 
             data = full)

# Draw discrimination line
np <- 300
nd.x <- seq(
  from = min(full$X1),
  to = max(full$X1),
  length.out = np
)
nd.y <- seq(
  from = min(full$X2),
  to = max(full$X2),
  length.out = np
)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)

prd <- as.numeric(predict(mylda, newdata = nd)$class)

plot(full[, 1:2], col = full$Response, main = "Shift in X2")

points(
  mylda$means,
  pch = "+",
  cex = 2,
  col = c("black", "red")
)

contour(
  x = nd.x,
  y = nd.y,
  z = matrix(prd, nrow = np, ncol = np),
  levels = c(1, 2),
  add = TRUE,
  drawlabels = FALSE
)
```

**The number of terms in the prior statement is determined by the number of classes in the LDA statement; in this case we have X1 and X2, so we need two terms in ours. Depending on the order of terms `prior = c(0.2, 0.8)` vs. `prior = c(0.8, 0.2)`, the diving line will shift towards one group or the other.**

## Exercise 2: LDA is not Robust Against Outliers?

### Question 7

Using the data set generated in number 5, add an additional data point (X1=11,X2=0,Response="Yes"). You can do this simply by doing something like this.

```{r, echo=T}
full[61, 1] <- 11
full[61, 2] <- 0
full[61, 3] <- "Yes"
```
Provide the LDA prediction boundary (no priors this time) for this data set with the additional point. Verify that the observation is an outlier. Does the prediction boundary change? 

```{r}
# Construct the LDA model
mylda <- lda(Response ~ X1 + X2, 
             data = full)

# Draw discrimination line
np <- 300
nd.x <- seq(
  from = min(full$X1),
  to = max(full$X1),
  length.out = np
)
nd.y <- seq(
  from = min(full$X2),
  to = max(full$X2),
  length.out = np
)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)

prd <- as.numeric(predict(mylda, newdata = nd)$class)

plot(full[, 1:2], col = full$Response, main = "Shift in X2")

points(
  mylda$means,
  pch = "+",
  cex = 2,
  col = c("black", "red")
)

contour(
  x = nd.x,
  y = nd.y,
  z = matrix(prd, nrow = np, ncol = np),
  levels = c(1, 2),
  add = TRUE,
  drawlabels = FALSE
)
```

**There is indeed a new, red outlier at `0, 11` and it appears that the prediction boundary has also shifted. It is now steeper and seems like it has increased the prediction accuracy as well based on the number of incorrect points on either side of the line.**

## Exercise 3: LDA and QDA are susceptible to overfitting

The following exercise will illustrate the fact that like any predictive model, the fears of over and under fitting are still present for LDA. It is often referred to as the "curse of dimensionality" in multivariate statistics. In addition to this concept, the following scripts will provide a good starting point in producing training and test set splits and generation of confusion matrices to assess model accuracy.

Let's first produce simple data sets like we have done for the previous exercises. These will serve as a training and test set. However this time, we are going to create 20 predictors rather than just 2. The first 2 predictors will be the actual significant predictors that can help in making predictions, the remaining 28 are just random and have no ability to predict the response. The reader is encouraged to explore the data set in scatter plots and can verify that indeed X1 and X2 are the predictors in which separation between the Yes observations and No's exist. 

```{r}
library(mvtnorm)
set.seed(1234)
muYes <- c(10, 10)
muNo <- c(8, 8)
Sigma <- matrix(c(1, .8, .8, 1), 2, 2, byrow = T)
nY <- 30
nN <- 30
dataYes <- rmvnorm(nY, muYes, Sigma)
dataNo <- rmvnorm(nN, muNo, Sigma)
train <- rbind(dataYes, dataNo)
train <- data.frame(train)
for (i in 3:20) {
  train <- cbind(train, rnorm(nY + nN))
}
names(train) <- paste("X", 1:20, sep = "")

train$Response <- rep(c("Yes", "No"), each = 30)
train$Response <- factor(train$Response)

# Creating a test set
muYes <- c(10, 10)
muNo <- c(8, 8)
Sigma <- matrix(c(1, .8, .8, 1), 2, 2, byrow = T)
nY <- 500
nN <- 500
dataYes <- rmvnorm(nY, muYes, Sigma)
dataNo <- rmvnorm(nN, muNo, Sigma)
test <- rbind(dataYes, dataNo)
test <- data.frame(test)
for (i in 3:20) {
  test <- cbind(test, rnorm(nY + nN))
}
names(test) <- paste("X", 1:20, sep = "")

test$Response <- rep(c("Yes", "No"), each = 500)
test$Response <- factor(test$Response)
```

For this example the training data set is small, while the test set is much bigger to help get a good feel for the accuracy that the models we can create produce. For example, suppose after doing some exploration that we started with model that just contained the first two predictors X1 and X2. The actual important predictors.

The following code runs a simple LDA on the training data set, predicts the observations on the test set, and produces a confusion matrix to see how well the predictions performed. Given that we are dealing with the perfect scenario here, including exactly what the truth is, our prediction accuracy on the test set should be pretty decent.

```{r}
mylda <- lda(Response ~ X1 + X2, data = train)

# Predictions can come in many forms, the class form provides the categorical level of your response.
pred <- predict(mylda, newdata = test)$class 
Truth <- test$Response

# Creating a confusion matrix
x <- table(pred, Truth) 
x

# Misclassification Error
ME <- (x[2, 1] + x[1, 2]) / 1000
ME

# Calculating overall accuracy
1 - ME
```
So the overall prediction accuracy is pretty good as expected, around 83%. 

### Question 8

Refit the LDA model using the first 10 predictors (X1-X10) and then again using all 20 predictors. For each of the model fits produce the confusion matrix on the test set and the overall accuracy. Does including unnecessary predictors damages LDA's ability to predict future values?

```{r}
set.seed(1234)
muYes <- c(10, 10)
muNo <- c(8, 8)
Sigma <- matrix(c(1, .8, .8, 1), 2, 2, byrow = T)
nY <- 30
nN <- 30
dataYes <- rmvnorm(nY, muYes, Sigma)
dataNo <- rmvnorm(nN, muNo, Sigma)
train <- rbind(dataYes, dataNo)
train <- data.frame(train)
for (i in 3:10) {
  train <- cbind(train, rnorm(nY + nN))
}
names(train) <- paste("X", 1:10, sep = "")

train$Response <- rep(c("Yes", "No"), each = 30)
train$Response <- factor(train$Response)

# Creating a test set
muYes <- c(10, 10)
muNo <- c(8, 8)
Sigma <- matrix(c(1, .8, .8, 1), 2, 2, byrow = T)
nY <- 500
nN <- 500
dataYes <- rmvnorm(nY, muYes, Sigma)
dataNo <- rmvnorm(nN, muNo, Sigma)
test <- rbind(dataYes, dataNo)
test <- data.frame(test)
for (i in 3:10) {
  test <- cbind(test, rnorm(nY + nN))
}
names(test) <- paste("X", 1:10, sep = "")

test$Response <- rep(c("Yes", "No"), each = 500)
test$Response <- factor(test$Response)

mylda <- lda(Response ~ X1 + X2, data = train)

# Predictions can come in many forms, the class form provides the categorical level of your response.
pred <- predict(mylda, newdata = test)$class 
Truth <- test$Response

# Creating a confusion matrix
x <- table(pred, Truth) 
x

# Misclassification Error
ME <- (x[2, 1] + x[1, 2]) / 1000
ME

# Calculating overall accuracy
1 - ME
```

**Using 10 predictors, we get an accuracy of 84%.**

```{r}
set.seed(1234)
muYes <- c(10, 10)
muNo <- c(8, 8)
Sigma <- matrix(c(1, .8, .8, 1), 2, 2, byrow = T)
nY <- 30
nN <- 30
dataYes <- rmvnorm(nY, muYes, Sigma)
dataNo <- rmvnorm(nN, muNo, Sigma)
train <- rbind(dataYes, dataNo)
train <- data.frame(train)
for (i in 3:30) {
  train <- cbind(train, rnorm(nY + nN))
}
names(train) <- paste("X", 1:30, sep = "")

train$Response <- rep(c("Yes", "No"), each = 30)
train$Response <- factor(train$Response)

# Creating a test set
muYes <- c(10, 10)
muNo <- c(8, 8)
Sigma <- matrix(c(1, .8, .8, 1), 2, 2, byrow = T)
nY <- 500
nN <- 500
dataYes <- rmvnorm(nY, muYes, Sigma)
dataNo <- rmvnorm(nN, muNo, Sigma)
test <- rbind(dataYes, dataNo)
test <- data.frame(test)
for (i in 3:30) {
  test <- cbind(test, rnorm(nY + nN))
}
names(test) <- paste("X", 1:30, sep = "")

test$Response <- rep(c("Yes", "No"), each = 500)
test$Response <- factor(test$Response)

mylda <- lda(Response ~ X1 + X2, data = train)

# Predictions can come in many forms, the class form provides the categorical level of your response.
pred <- predict(mylda, newdata = test)$class 
Truth <- test$Response

# Creating a confusion matrix
x <- table(pred, Truth) 
x

# Misclassification Error
ME <- (x[2, 1] + x[1, 2]) / 1000
ME

# Calculating overall accuracy
1 - ME
```

**Using all 30 predictors, we get 85.6% accuracy.**

## Additional examples to play with. No more assignments.

### QDA

Running QDA instead of LDA is simple enough. Just use the qda function instead of the lda function. Everything else is the same. It is important to realize that QDA includes more parameters in the actual model fit so it can potentially overfit a data set as well if the assumptions of LDA are met. However, if the sample sizes are large relative to the number of predictors being included in the model, then QDA suffers little from overfitting.

This can be illustrated from the following example. Here we simulate multiple data sets that are consistent with LDA assumptions. For each data set we increase the sample size. QDA is applied to each of the data sets and the prediction boundary is provided. The reader can verify that the QDA prediction boundary becomes more and more linear as the sample size increases and the information in the data more strongly agrees with the LDA assumption.

The code producing these results is rather lengthy. You can refer to the R markdown file to run yourself.

```{r, echo=FALSE, fig.height=3,fig.width=8}
par(mfrow = c(1, 4))

library(MASS)

# Another scenario
set.seed(12)
dataYes <- mvrnorm(15, c(10, 10), matrix(c(1, .6, .6, 1), 2, 2, byrow = T))
dataNo <- mvrnorm(15, c(8, 8), matrix(c(1, .6, .6, 1), 2, 2, byrow = T))
full <- rbind(dataYes, dataNo)
full <- data.frame(full)
full$Response <- rep(c("Yes", "No"), each = 15)
full$Response <- factor(full$Response)
names(full)[1:2] <- c("X1", "X2")

# Construct the model
mylda <- qda(Response ~ X1 + X2, data = full)

# Draw discrimination line
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)

prd <- as.numeric(predict(mylda, newdata = nd)$class)

plot(full[, 1:2], col = full$Response, main = "15 obs/group")
points(mylda$means, pch = "+", cex = 2, col = c("black", "red"))
contour(
  x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np),
  levels = c(1, 2), add = TRUE, drawlabels = FALSE
)

# Another scenario
dataYes <- mvrnorm(30, c(10, 10), matrix(c(1, .6, .6, 1), 2, 2, byrow = T))
dataNo <- mvrnorm(30, c(8, 8), matrix(c(1, .6, .6, 1), 2, 2, byrow = T))
full <- rbind(dataYes, dataNo)
full <- data.frame(full)
full$Response <- rep(c("Yes", "No"), each = 30)
full$Response <- factor(full$Response)
names(full)[1:2] <- c("X1", "X2")

# Construct the model
mylda <- qda(Response ~ X1 + X2, data = full)

# Draw discrimination line
np <- 300
nd.x <- seq(
  from = min(full$X1),
  to = max(full$X1),
  length.out = np
)
nd.y <- seq(
  from = min(full$X2),
  to = max(full$X2),
  length.out = np
)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)

prd <- as.numeric(predict(mylda, newdata = nd)$class)

plot(full[, 1:2], col = full$Response, main = "30 obs/group")
points(
  mylda$means,
  pch = "+",
  cex = 2,
  col = c("black", "red")
)
contour(
  x = nd.x,
  y = nd.y,
  z = matrix(prd, nrow = np, ncol = np),
  levels = c(1, 2),
  add = TRUE,
  drawlabels = FALSE
)

# Another scenario
dataYes <- mvrnorm(300, c(10, 10), matrix(c(1, .6, .6, 1), 2, 2, byrow = T))
dataNo <- mvrnorm(300, c(8, 8), matrix(c(1, .6, .6, 1), 2, 2, byrow = T))
full <- rbind(dataYes, dataNo)
full <- data.frame(full)
full$Response <- rep(c("Yes", "No"), each = 300)
full$Response <- factor(full$Response)
names(full)[1:2] <- c("X1", "X2")

# construct the model
mylda <- qda(Response ~ X1 + X2, data = full)

# draw discrimination line
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)

prd <- as.numeric(predict(mylda, newdata = nd)$class)

plot(full[, 1:2], col = full$Response, main = "300 obs/group")
points(
  mylda$means,
  pch = "+",
  cex = 2,
  col = c("black", "red")
)
contour(
  x = nd.x,
  y = nd.y,
  z = matrix(prd, nrow = np, ncol = np),
  levels = c(1, 2),
  add = TRUE,
  drawlabels = FALSE
)

# Another scenario
dataYes <- mvrnorm(3000, c(10, 10), matrix(c(1, .6, .6, 1), 2, 2, byrow = T))
dataNo <- mvrnorm(3000, c(8, 8), matrix(c(1, .6, .6, 1), 2, 2, byrow = T))
full <- rbind(dataYes, dataNo)
full <- data.frame(full)
full$Response <- rep(c("Yes", "No"), each = 3000)
full$Response <- factor(full$Response)
names(full)[1:2] <- c("X1", "X2")

# Construct the model
mylda <- qda(Response ~ X1 + X2, data = full)

# Draw discrimination line
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)

prd <- as.numeric(predict(mylda, newdata = nd)$class)

plot(full[, 1:2], col = full$Response, main = "3000 obs/group")
points(
  mylda$means,
  pch = "+",
  cex = 2,
  col = c("black", "red")
)
contour(
  x = nd.x,
  y = nd.y,
  z = matrix(prd, nrow = np, ncol = np),
  levels = c(1, 2),
  add = TRUE,
  drawlabels = FALSE
)
```

### When QDA Bests LDA

In the example below, here the LDA assumptions are violated since the red population is negatively correlated while the black population is positively correlated (equal cov is not met). QDA is more appropriate here as the quadratic curvature in the prediction boundary reflects the different correlation structures.

```{r, echo=F}
par(mfrow = c(1, 2))
set.seed(1234)
dataYes <- mvrnorm(100, c(10, 10), matrix(c(1, -.6, -.6, 1), 2, 2, byrow = T))
dataNo <- mvrnorm(100, c(8, 8), matrix(c(1, .6, .6, 1), 2, 2, byrow = T))
full <- rbind(dataYes, dataNo)
full <- data.frame(full)
full$Response <- rep(c("Yes", "No"), each = 100)
full$Response <- factor(full$Response)
names(full)[1:2] <- c("X1", "X2")

# construct the model
mylda <- qda(Response ~ X1 + X2, data = full)

# draw discrimination line
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)

prd <- as.numeric(predict(mylda, newdata = nd)$class)

plot(full[, 1:2], col = full$Response, main = "QDA")
points(
  mylda$means,
  pch = "+",
  cex = 2,
  col = c("black", "red")
)
contour(
  x = nd.x,
  y = nd.y,
  z = matrix(prd, nrow = np, ncol = np),
  levels = c(1, 2),
  add = TRUE,
  drawlabels = FALSE
)

set.seed(1234)
dataYes <- mvrnorm(100, c(10, 10), matrix(c(1, -.6, -.6, 1), 2, 2, byrow = T))
dataNo <- mvrnorm(100, c(8, 8), matrix(c(1, .6, .6, 1), 2, 2, byrow = T))
full <- rbind(dataYes, dataNo)
full <- data.frame(full)
full$Response <- rep(c("Yes", "No"), each = 100)
full$Response <- factor(full$Response)
names(full)[1:2] <- c("X1", "X2")

# construct the model
mylda <- lda(Response ~ X1 + X2, data = full)

# draw discrimination line
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)

prd <- as.numeric(predict(mylda, newdata = nd)$class)

plot(full[, 1:2], col = full$Response, main = "LDA")
points(
  mylda$means,
  pch = "+",
  cex = 2,
  col = c("black", "red")
)
contour(
  x = nd.x,
  y = nd.y,
  z = matrix(prd, nrow = np, ncol = np),
  levels = c(1, 2),
  add = TRUE,
  drawlabels = FALSE
)
```

Since the black population is positively correlated versus the red being negative, you can see that for extremely large values of x1 and x2, the prediction will be for the black population, something that LDA will not do.

Although not realistic, here is another silly example illustrating when QDA is gonna best LDA.

```{r, echo=F}
# Another scenario
par(mfrow = c(1, 2))
set.seed(1234)
dataYes <- mvrnorm(100, c(10, 10), matrix(c(1, -.8, -.8, 1), 2, 2, byrow = T))
dataNo <- mvrnorm(100, c(10, 10), matrix(c(1, .5, .5, 1), 2, 2, byrow = T))
full <- rbind(dataYes, dataNo)
full <- data.frame(full)
full$Response <- rep(c("Yes", "No"), each = 100)
full$Response <- factor(full$Response)
names(full)[1:2] <- c("X1", "X2")

# construct the model
mylda <- qda(Response ~ X1 + X2, data = full)

# draw discrimination line
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)

prd <- as.numeric(predict(mylda, newdata = nd)$class)

plot(full[, 1:2], col = full$Response, main = "QDA")
points(
  mylda$means,
  pch = "+",
  cex = 2,
  col = c("black", "red")
)
contour(
  x = nd.x,
  y = nd.y,
  z = matrix(prd, nrow = np, ncol = np),
  levels = c(1, 2),
  add = TRUE,
  drawlabels = FALSE
)

set.seed(1234)
dataYes <- mvrnorm(100, c(10, 10), matrix(c(1, -.8, -.8, 1), 2, 2, byrow = T))
dataNo <- mvrnorm(100, c(10, 10), matrix(c(1, .5, .5, 1), 2, 2, byrow = T))
full <- rbind(dataYes, dataNo)
full <- data.frame(full)
full$Response <- rep(c("Yes", "No"), each = 100)
full$Response <- factor(full$Response)
names(full)[1:2] <- c("X1", "X2")

# construct the model
mylda <- lda(Response ~ X1 + X2, data = full)

# draw discrimination line
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)

prd <- as.numeric(predict(mylda, newdata = nd)$class)

plot(full[, 1:2], col = full$Response, main = "LDA")
points(
  mylda$means,
  pch = "+",
  cex = 2,
  col = c("black", "red")
)
contour(
  x = nd.x,
  y = nd.y,
  z = matrix(prd, nrow = np, ncol = np),
  levels = c(1, 2),
  add = TRUE,
  drawlabels = FALSE
)
```
